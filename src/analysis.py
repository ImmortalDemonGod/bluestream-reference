"""
analysis.py - Virtual triangulation matching algorithm

Expected runtime: < 1 minute (uses spatial indexing)
Expected output: data/outputs/matched_pairs.csv (25 records, Phase 2 volunteer validation)
"""

import pandas as pd
import numpy as np
from scipy import stats, spatial
from pathlib import Path
from tqdm import tqdm
import yaml
import requests

def load_config():
    """Load configuration"""
    with open('config/config.yaml', 'r') as f:
        return yaml.safe_load(f)

def haversine_distance(lat1, lon1, lat2, lon2):
    """
    Calculate great-circle distance between two points on Earth
    
    Args:
        lat1, lon1: First point (decimal degrees)
        lat2, lon2: Second point (decimal degrees)
        
    Returns:
        Distance in meters
    """
    
    # Earth's radius in meters
    R = 6371000
    
    # Convert to radians
    lat1_rad = np.radians(lat1)
    lat2_rad = np.radians(lat2)
    lon1_rad = np.radians(lon1)
    lon2_rad = np.radians(lon2)
    
    # Differences
    dlat = lat2_rad - lat1_rad
    dlon = lon2_rad - lon1_rad
    
    # Haversine formula
    a = np.sin(dlat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon/2)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
    
    return R * c

def find_matches(volunteer_df, professional_df, config):
    """
    Find volunteer-professional measurement pairs using spatial-temporal matching
    
    Matching criteria (read from config.yaml):
    1. Distance <= max_distance_meters (Haversine)
    2. Time difference <= max_time_hours (absolute)
    3. match_strategy: 'closest' = nearest in space, 'all' = all qualifying
    
    Args:
        volunteer_df: Volunteer measurements
        professional_df: Professional measurements
        config: Configuration dictionary
        
    Returns:
        DataFrame with matched pairs (EXACT column names matter!)
    """
    
    params = config['matching_parameters']
    max_distance_m = params['max_distance_meters']
    max_time_hours = params['max_time_hours']
    match_strategy = params['match_strategy']
    
    matches = []
    
    print(f"\nMatching volunteer measurements to professional...")
    print(f"  Volunteer measurements: {len(volunteer_df):,}")
    print(f"  Professional measurements: {len(professional_df):,}")
    print(f"  Max distance: {max_distance_m}m")
    print(f"  Max time: {max_time_hours}hrs")
    print(f"\nUsing cKDTree for spatial indexing...")
    
    # Prepare data for cKDTree
    vol_coords = volunteer_df[['LatitudeMeasure', 'LongitudeMeasure']].values
    pro_coords = professional_df[['LatitudeMeasure', 'LongitudeMeasure']].values
    
    # Convert professional dataframe to records for O(1) access
    # This is CRITICAL for performance. iloc inside a loop is too slow.
    pro_records = professional_df.to_dict('records')
    
    # Build Tree on Professional Data
    tree = spatial.cKDTree(pro_coords)
    
    # Query for candidates within ~250 meters (approx 0.0025 degrees) to be safe
    # 1 degree lat ~ 111km. 100m = 0.0009 deg. Using 0.0025 as buffer.
    search_radius_deg = 0.0025
    
    # query_ball_point returns list of indices for each volunteer point
    indices_list = tree.query_ball_point(vol_coords, search_radius_deg)
    
    # Iterate through volunteer measurements
    for idx, pro_indices in tqdm(enumerate(indices_list), 
                                 total=len(volunteer_df), 
                                 desc="Matching"):
        
        if not pro_indices:
            continue
            
        vol_row = volunteer_df.iloc[idx]
        
        # Extract volunteer details
        vol_lat = vol_row['LatitudeMeasure']
        vol_lon = vol_row['LongitudeMeasure']
        vol_datetime = vol_row['ActivityStartDate']
        vol_value = vol_row['ResultMeasureValue']
        vol_site_id = vol_row['MonitoringLocationIdentifier']
        vol_org = vol_row['OrganizationIdentifier']

        vol_units = np.nan
        if 'ResultMeasure/MeasureUnitCode' in volunteer_df.columns:
            vol_units = vol_row['ResultMeasure/MeasureUnitCode']

        if match_strategy == 'all':
            # Append all qualifying matches (reference-style behavior)
            for pro_idx in pro_indices:
                pro_row = pro_records[pro_idx]

                # Calculate EXACT Haversine distance
                distance = haversine_distance(
                    vol_lat,
                    vol_lon,
                    pro_row['LatitudeMeasure'],
                    pro_row['LongitudeMeasure'],
                )

                if distance > max_distance_m:
                    continue

                # Calculate temporal difference
                time_diff = abs((pro_row['ActivityStartDate'] - vol_datetime).total_seconds() / 3600)

                if time_diff > max_time_hours:
                    continue

                pro_units = np.nan
                if 'ResultMeasure/MeasureUnitCode' in professional_df.columns:
                    pro_units = pro_row.get('ResultMeasure/MeasureUnitCode', np.nan)

                pro_method = pro_row.get('ResultAnalyticalMethod/MethodIdentifier', '')
                matches.append({
                    'Vol_SiteID': vol_site_id,
                    'Pro_SiteID': pro_row['MonitoringLocationIdentifier'],
                    'Vol_Organization': vol_org,
                    'Pro_Organization': pro_row['OrganizationIdentifier'],
                    'Pro_Method_ID': pro_method if pd.notna(pro_method) else '',
                    'Vol_Value': vol_value,
                    'Pro_Value': pro_row['ResultMeasureValue'],
                    'Vol_Units': vol_units,
                    'Pro_Units': pro_units,
                    'Vol_DateTime': vol_datetime,
                    'Pro_DateTime': pro_row['ActivityStartDate'],
                    'Vol_Lat': vol_lat,
                    'Vol_Lon': vol_lon,
                    'Pro_Lat': pro_row['LatitudeMeasure'],
                    'Pro_Lon': pro_row['LongitudeMeasure'],
                    'Distance_m': distance,
                    'Time_Diff_hours': time_diff,
                })

            continue

        candidates = []
        
        # Check specific candidates from spatial index
        for pro_idx in pro_indices:
            pro_row = pro_records[pro_idx]
            
            # Calculate EXACT Haversine distance
            distance = haversine_distance(vol_lat, vol_lon, 
                                        pro_row['LatitudeMeasure'], 
                                        pro_row['LongitudeMeasure'])
            
            if distance > max_distance_m:
                continue
                
            # Calculate temporal difference
            time_diff = abs((pro_row['ActivityStartDate'] - vol_datetime).total_seconds() / 3600)
            
            if time_diff <= max_time_hours:
                pro_units = np.nan
                if 'ResultMeasure/MeasureUnitCode' in professional_df.columns:
                    pro_units = pro_row.get('ResultMeasure/MeasureUnitCode', np.nan)
                pro_method = pro_row.get('ResultAnalyticalMethod/MethodIdentifier', '')
                candidates.append({
                    'distance': distance,
                    'time_diff': time_diff,
                    'pro_value': pro_row['ResultMeasureValue'],
                    'pro_units': pro_units,
                    'pro_org': pro_row['OrganizationIdentifier'],
                    'pro_method': pro_method if pd.notna(pro_method) else '',
                    'pro_site_id': pro_row['MonitoringLocationIdentifier'],
                    'pro_datetime': pro_row['ActivityStartDate'],
                    'pro_lat': pro_row['LatitudeMeasure'],
                    'pro_lon': pro_row['LongitudeMeasure']
                })
        
        # If we found matches, select according to strategy
        if len(candidates) > 0:
            if match_strategy == 'closest_time':
                # Sort by time difference, then distance
                candidates.sort(key=lambda x: (x['time_diff'], x['distance']))
            else:
                # Default to spatially closest
                candidates.sort(key=lambda x: x['distance'])
            best_match = candidates[0]
            
            matches.append({
                'Vol_SiteID': vol_site_id,
                'Pro_SiteID': best_match['pro_site_id'],
                'Vol_Organization': vol_org,
                'Pro_Organization': best_match['pro_org'],
                'Pro_Method_ID': best_match['pro_method'],
                'Vol_Value': vol_value,
                'Pro_Value': best_match['pro_value'],
                'Vol_Units': vol_units,
                'Pro_Units': best_match['pro_units'],
                'Vol_DateTime': vol_datetime,
                'Pro_DateTime': best_match['pro_datetime'],
                'Vol_Lat': vol_lat,
                'Vol_Lon': vol_lon,
                'Pro_Lat': best_match['pro_lat'],
                'Pro_Lon': best_match['pro_lon'],
                'Distance_m': best_match['distance'],
                'Time_Diff_hours': best_match['time_diff']
            })
    
    return pd.DataFrame(matches)

def calculate_statistics(matches_df):
    """Calculate correlation and regression statistics"""
    
    vol_values = matches_df['Vol_Value'].values
    pro_values = matches_df['Pro_Value'].values
    
    slope, intercept, r_value, p_value, std_err = stats.linregress(pro_values, vol_values)
    r_squared = r_value ** 2
    
    result = {
        'n': len(matches_df),
        'r_squared': r_squared,
        'slope': slope,
        'intercept': intercept,
        'p_value': p_value,
    }
    if 'Vol_SiteID' in matches_df.columns:
        result['n_unique_sites'] = matches_df['Vol_SiteID'].nunique()
    return result

def save_results(matches_df, stats, config):
    """Save matched pairs and statistics"""
    
    output_dir = Path(config['output_paths']['results'])
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Save matched pairs
    matches_path = output_dir / "matched_pairs.csv"
    matches_df.to_csv(matches_path, index=False)
    
    # Compute site diversity metrics
    n_vol_sites = matches_df['Vol_SiteID'].nunique() if len(matches_df) > 0 else 0
    n_pro_sites = matches_df['Pro_SiteID'].nunique() if len(matches_df) > 0 else 0
    n_pro_orgs = matches_df['Pro_Organization'].nunique() if len(matches_df) > 0 else 0
    pro_org_breakdown = ""
    if len(matches_df) > 0:
        for org, count in matches_df['Pro_Organization'].value_counts().items():
            pro_org_breakdown += f"  {org}: {count} matches\n"

    # Save statistics
    stats_path = output_dir / "summary_statistics.txt"
    with open(stats_path, 'w') as f:
        f.write("Blue Thumb Virtual Triangulation Results\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"Sample Size: N = {stats['n']}\n")
        f.write(f"Correlation: R² = {stats['r_squared']:.3f}\n")
        f.write(f"Slope: {stats['slope']:.3f}\n")
        f.write(f"Intercept: {stats['intercept']:.3f}\n")
        f.write(f"P-value: {stats['p_value']:.4e}\n")
        f.write(f"\nSite Diversity\n")
        f.write(f"-" * 30 + "\n")
        f.write(f"Unique volunteer sites: {n_vol_sites}\n")
        f.write(f"Unique professional sites: {n_pro_sites}\n")
        f.write(f"Professional organizations: {n_pro_orgs}\n")
        if pro_org_breakdown:
            f.write(pro_org_breakdown)
    
    print(f"\nResults saved:")
    print(f"  {matches_path}")
    print(f"  {stats_path}")
    
    return matches_path, stats_path


from src.utils.station_lookup import fetch_wqp_station_profiles  # noqa: F401
from src.utils.station_lookup import station_name_lookup_from_matched_pairs  # noqa: F401

def run_comparison(label, test_df, professional_df, config, output_prefix):
    """Run a single matching comparison and save results.

    Args:
        label: Human-readable label (e.g. 'Pro-to-Pro', 'Vol-to-Pro')
        test_df: The 'test' side DataFrame (rotating basin OR Blue Thumb)
        professional_df: The professional reference DataFrame
        config: Configuration dictionary
        output_prefix: Filename prefix for outputs (e.g. 'pro_to_pro', 'vol_to_pro')

    Returns:
        (matches_df, stats_dict) or (empty DataFrame, None) if no matches
    """
    print(f"\n{'='*60}")
    print(f"  {label}")
    print(f"{'='*60}")

    matches_df = find_matches(test_df, professional_df, config)
    if len(matches_df) == 0:
        print(f"  No matches found for {label}.")
        return matches_df, None

    st = calculate_statistics(matches_df)

    print(f"\n  N = {st['n']}")
    print(f"  R² = {st['r_squared']:.3f}")
    print(f"  Slope = {st['slope']:.3f}")
    print(f"  P-value = {st['p_value']:.4e}")

    # Save with prefixed filenames
    output_dir = Path(config['output_paths']['results'])
    output_dir.mkdir(parents=True, exist_ok=True)

    matches_path = output_dir / f"matched_pairs_{output_prefix}.csv"
    matches_df.to_csv(matches_path, index=False)

    stats_path = output_dir / f"summary_statistics_{output_prefix}.txt"
    with open(stats_path, 'w') as f:
        f.write(f"Blue Thumb Virtual Triangulation — {label}\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"Sample Size: N = {st['n']}\n")
        f.write(f"Correlation: R² = {st['r_squared']:.3f}\n")
        f.write(f"Slope: {st['slope']:.3f}\n")
        f.write(f"Intercept: {st['intercept']:.3f}\n")
        f.write(f"P-value: {st['p_value']:.4e}\n")
        n_test_sites = matches_df['Vol_SiteID'].nunique()
        n_pro_sites = matches_df['Pro_SiteID'].nunique()
        n_pro_orgs = matches_df['Pro_Organization'].nunique()
        f.write(f"\nUnique test sites: {n_test_sites}\n")
        f.write(f"Unique professional sites: {n_pro_sites}\n")
        f.write(f"Professional organizations: {n_pro_orgs}\n")
        for org, count in matches_df['Pro_Organization'].value_counts().items():
            f.write(f"  {org}: {count} matches\n")

        # Per-organization regression breakdown
        f.write(f"\n{'='*50}\n")
        f.write("Per-Organization Breakdown\n")
        f.write(f"{'='*50}\n\n")
        for org in matches_df['Pro_Organization'].unique():
            subset = matches_df[matches_df['Pro_Organization'] == org]
            n_org = len(subset)
            if n_org >= 3:
                org_st = calculate_statistics(subset)
                f.write(f"{org} (N={n_org}):\n")
                f.write(f"  R² = {org_st['r_squared']:.3f}\n")
                f.write(f"  Slope = {org_st['slope']:.3f}\n")
                f.write(f"  P-value = {org_st['p_value']:.4e}\n")
            else:
                f.write(f"{org} (N={n_org}): Too few matches for regression\n")
            f.write("\n")

        # Method provenance from actual matched data
        f.write(f"{'='*50}\n")
        f.write("Analytical Method Provenance\n")
        f.write(f"{'='*50}\n\n")
        if 'Pro_Method_ID' in matches_df.columns:
            n_total = len(matches_df)
            n_empty = (matches_df['Pro_Method_ID'].fillna('') == '').sum()
            n_known = n_total - n_empty
            f.write(f"Matches with known method: {n_known}/{n_total} ({100*n_known/n_total:.0f}%)\n")
            f.write(f"Matches with UNKNOWN method: {n_empty}/{n_total} ({100*n_empty/n_total:.0f}%)\n\n")
            if n_empty > 0:
                f.write(f"⚠ WARNING: {n_empty} of {n_total} matches ({100*n_empty/n_total:.0f}%) have\n")
                f.write(f"  undefined professional methodology in WQP metadata.\n")
                empty_orgs = matches_df[matches_df['Pro_Method_ID'].fillna('') == '']['Pro_Organization'].value_counts()
                for org, cnt in empty_orgs.items():
                    f.write(f"  - {org}: {cnt} matches with no method recorded\n")
                f.write("\n")
            for method_id, cnt in matches_df[matches_df['Pro_Method_ID'].fillna('') != '']['Pro_Method_ID'].value_counts().items():
                f.write(f"Method '{method_id}': {cnt} matches\n")
        else:
            f.write("Pro_Method_ID column not available.\n")
        f.write("\n")
        f.write("Reference:\n")
        f.write("  OKWRB-STREAMS_WQX: EPA 325.1/325.2 (Automated Colorimetry) — VERIFIED\n")
        f.write("  CNENVSER: Method UNKNOWN — contact to confirm before citing.\n")

    print(f"\n  Saved: {matches_path}")
    print(f"  Saved: {stats_path}")

    return matches_df, st


def run_spatial_coverage(volunteer_df, professional_df, v2p_matches, config):
    """Quantify how many volunteer sites are far from any professional monitor.

    Saves data/outputs/spatial_coverage_analysis.txt.
    """
    print(f"\n{'='*60}")
    print(f"  SPATIAL COVERAGE ANALYSIS")
    print(f"{'='*60}")

    vol_sites = volunteer_df.groupby('MonitoringLocationIdentifier').agg(
        lat=('LatitudeMeasure', 'first'),
        lon=('LongitudeMeasure', 'first')
    )
    pro_sites = professional_df.groupby('MonitoringLocationIdentifier').agg(
        lat=('LatitudeMeasure', 'first'),
        lon=('LongitudeMeasure', 'first')
    )

    if len(pro_sites) == 0 or len(vol_sites) == 0:
        print("  Insufficient data for coverage analysis.")
        return

    pro_tree = spatial.cKDTree(pro_sites[['lat', 'lon']].values)
    vol_coords = vol_sites[['lat', 'lon']].values
    dd, _ = pro_tree.query(vol_coords, k=1)
    dist_km = dd * 111.0  # approximate degree-to-km

    n_total = len(vol_sites)
    thresholds = [1, 5, 10, 25, 50]
    print(f"\n  Total Blue Thumb volunteer sites: {n_total}")
    print(f"  Total professional sites: {len(pro_sites)}")
    print(f"\n  Volunteer sites by distance to nearest professional monitor:")
    for t in thresholds:
        n_far = (dist_km > t).sum()
        print(f"    > {t:>2d} km: {n_far:>4d} sites ({100*n_far/n_total:.0f}%)")

    matched_sites = 0
    if v2p_matches is not None and len(v2p_matches) > 0:
        matched_sites = v2p_matches['Vol_SiteID'].nunique()
    print(f"\n  Sites with professional match (≤125m, ≤72h): {matched_sites}")
    print(f"  Sites without match: {n_total - matched_sites} ({100*(n_total-matched_sites)/n_total:.0f}%)")
    print(f"  → Volunteers monitor where professionals don't.")

    output_dir = Path(config['output_paths']['results'])
    coverage_path = output_dir / "spatial_coverage_analysis.txt"
    with open(coverage_path, 'w') as f:
        f.write("Blue Thumb Spatial Coverage Analysis\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"Total Blue Thumb volunteer sites: {n_total}\n")
        f.write(f"Total professional sites: {len(pro_sites)}\n\n")
        f.write("Distance from each volunteer site to nearest professional monitor:\n")
        for t in thresholds:
            n_far = (dist_km > t).sum()
            f.write(f"  > {t:>2d} km: {n_far:>4d} sites ({100*n_far/n_total:.0f}%)\n")
        f.write(f"\nSites with match (≤125m, ≤72h): {matched_sites}\n")
        f.write(f"Sites without match: {n_total - matched_sites} ({100*(n_total-matched_sites)/n_total:.0f}%)\n\n")
        f.write("Interpretation: The low overlap (N=25 matched pairs from 4 sites)\n")
        f.write("reflects the complementary nature of volunteer monitoring — Blue Thumb\n")
        f.write("volunteers primarily monitor locations that professionals do not reach.\n")
        f.write("This validates the program's coverage value, not a weakness of the study.\n")
    print(f"\n  Saved: {coverage_path}")


def main():
    """Run virtual triangulation analysis — both pro-to-pro and vol-to-pro."""

    config = load_config()
    proc_dir = Path(config['output_paths']['processed_data'])

    # Load professional reference data (same for both comparisons)
    professional_df = pd.read_csv(proc_dir / "professional_chloride.csv", low_memory=False)
    professional_df['ActivityStartDate'] = pd.to_datetime(professional_df['ActivityStartDate'])

    # --- Comparison 1: Pro-to-Pro (OCC Rotating Basin vs OKWRB/CNENVSER) ---
    rb_path = proc_dir / "rotating_basin_chloride.csv"
    if rb_path.exists():
        rotating_basin_df = pd.read_csv(rb_path, low_memory=False)
        rotating_basin_df['ActivityStartDate'] = pd.to_datetime(rotating_basin_df['ActivityStartDate'])
        p2p_matches, p2p_stats = run_comparison(
            "PRO-TO-PRO: OCC Rotating Basin vs Professional Reference",
            rotating_basin_df, professional_df, config, "pro_to_pro"
        )
    else:
        print(f"\n⚠️  {rb_path} not found — skipping pro-to-pro comparison.")
        print("   Run transform.py first to generate this file.")
        p2p_stats = None

    # --- Comparison 2: Vol-to-Pro (Blue Thumb vs OKWRB/CNENVSER) ---
    volunteer_df = pd.read_csv(proc_dir / "volunteer_chloride.csv", low_memory=False)
    volunteer_df['ActivityStartDate'] = pd.to_datetime(volunteer_df['ActivityStartDate'])
    v2p_matches, v2p_stats = run_comparison(
        "VOL-TO-PRO: Blue Thumb Volunteers vs Professional Reference",
        volunteer_df, professional_df, config, "vol_to_pro"
    )

    # Also save vol-to-pro as the default matched_pairs.csv for backward compatibility
    if len(v2p_matches) > 0:
        save_results(v2p_matches, v2p_stats, config)

    # --- Side-by-side summary ---
    print(f"\n{'='*60}")
    print(f"  SIDE-BY-SIDE COMPARISON")
    print(f"{'='*60}")
    print(f"{'Metric':<25} {'Pro-to-Pro':>15} {'Vol-to-Pro':>15}")
    print(f"{'-'*55}")
    if p2p_stats:
        print(f"{'N':<25} {p2p_stats['n']:>15} {v2p_stats['n'] if v2p_stats else 'N/A':>15}")
        print(f"{'R²':<25} {p2p_stats['r_squared']:>15.3f} {v2p_stats['r_squared'] if v2p_stats else 0:>15.3f}")
        print(f"{'Slope':<25} {p2p_stats['slope']:>15.3f} {v2p_stats['slope'] if v2p_stats else 0:>15.3f}")
    else:
        print("  Pro-to-pro data not available.")

    # --- Spatial Coverage Analysis ---
    run_spatial_coverage(volunteer_df, professional_df, v2p_matches, config)

    print("\n✅ Virtual triangulation analysis complete")

if __name__ == "__main__":
    main()
